{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIJ9tMPjPVErbz4DTt4/Og",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joseikwang/Time-Series/blob/main/Customer_Care_Chatbot_with_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step1: Set up**"
      ],
      "metadata": {
        "id": "dr2YPDTuYwTb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eJ10f0ZYd8v",
        "outputId": "3714ca61-3858-4cdb-97fc-ff563d97f5f8",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting streamlit-chat\n",
            "  Downloading streamlit_chat-0.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting pickle-mixin\n",
            "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit_chat-0.1.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pickle-mixin\n",
            "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=5988 sha256=cea02b0ffd35365b1a1c247c3d29f05d45e44cc7e9a663d80a3d8d44085376be\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/9c/0d/8709be17c02b72bf04ea60d8ec64fc46a1554c0fb81e048dd6\n",
            "Successfully built pickle-mixin\n",
            "Installing collected packages: pickle-mixin, watchdog, pydeck, streamlit, streamlit-chat\n",
            "Successfully installed pickle-mixin-1.0.2 pydeck-0.9.1 streamlit-1.44.1 streamlit-chat-0.1.1 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install streamlit streamlit-chat numpy tensorflow pickle-mixin\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step2: Import the chatbot codes**"
      ],
      "metadata": {
        "id": "Fp_uCs1RZSuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile customer_care_chatbot.py\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import json\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Load the pre-trained model and data\n",
        "@st.cache_resource\n",
        "def load_resources():\n",
        "    # Load the trained model\n",
        "    model = load_model('customer_care_gru_model.h5')\n",
        "\n",
        "    # Load tokenizer\n",
        "    with open('tokenizer.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "    # Load label encoder\n",
        "    with open('label_encoder.pickle', 'rb') as handle:\n",
        "        lbl_encoder = pickle.load(handle)\n",
        "\n",
        "    # Load intents\n",
        "    with open('intents.json') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return model, tokenizer, lbl_encoder, data\n",
        "\n",
        "# Function to get bot response\n",
        "def get_response(model, tokenizer, lbl_encoder, intents, text):\n",
        "    # Predict the intent\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(sequence, truncating='post', maxlen=20)\n",
        "    pred = model.predict(padded)\n",
        "    intent = lbl_encoder.inverse_transform([np.argmax(pred)])[0]\n",
        "\n",
        "    # Get a random response for the predicted intent\n",
        "    for i in intents['intents']:\n",
        "        if i['tag'] == intent:\n",
        "            result = np.random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "\n",
        "# Main Streamlit app\n",
        "def main():\n",
        "    st.title(\"Customer Care Chatbot\")\n",
        "    st.write(\"Hello! I'm your customer care assistant. How can I help you today?\")\n",
        "\n",
        "    # Initialize session state for chat history\n",
        "    if 'generated' not in st.session_state:\n",
        "        st.session_state['generated'] = []\n",
        "    if 'past' not in st.session_state:\n",
        "        st.session_state['past'] = []\n",
        "\n",
        "    # Load model and data\n",
        "    model, tokenizer, lbl_encoder, intents = load_resources()\n",
        "\n",
        "    # Chat input\n",
        "    def get_text():\n",
        "        input_text = st.text_input(\"You: \", \"\", key=\"input\")\n",
        "        return input_text\n",
        "\n",
        "    user_input = get_text()\n",
        "\n",
        "    if user_input:\n",
        "        output = get_response(model, tokenizer, lbl_encoder, intents, user_input)\n",
        "        st.session_state.past.append(user_input)\n",
        "        st.session_state.generated.append(output)\n",
        "\n",
        "    # Display chat history\n",
        "    if st.session_state['generated']:\n",
        "        for i in range(len(st.session_state['generated'])-1, -1, -1):\n",
        "            message(st.session_state[\"generated\"][i], key=str(i))\n",
        "            message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0diW81BGZI0M",
        "outputId": "1375a918-ccc7-4ccc-b408-c392e194172f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing customer_care_chatbot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step3: Creat sample Data files**\n",
        "\n",
        "you will need to create sample data files"
      ],
      "metadata": {
        "id": "E0bgSee0Zt6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile intents.json\n",
        "{\n",
        "  \"intents\": [\n",
        "    {\n",
        "      \"tag\": \"greeting\",\n",
        "      \"patterns\": [\n",
        "        \"Hi\",\n",
        "        \"Hey\",\n",
        "        \"Hello\",\n",
        "        \"Good day\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"Hello! How can I help you today?\",\n",
        "        \"Hi there! What can I do for you?\",\n",
        "        \"Greetings! How may I assist you?\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"goodbye\",\n",
        "      \"patterns\": [\n",
        "        \"Bye\",\n",
        "        \"Goodbye\",\n",
        "        \"See you later\",\n",
        "        \"Have a nice day\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"Goodbye! Have a great day!\",\n",
        "        \"See you later! Thanks for visiting.\",\n",
        "        \"Bye! Come back again soon.\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"thanks\",\n",
        "      \"patterns\": [\n",
        "        \"Thanks\",\n",
        "        \"Thank you\",\n",
        "        \"That's helpful\",\n",
        "        \"Appreciate it\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"You're welcome!\",\n",
        "        \"Happy to help!\",\n",
        "        \"Glad I could assist you!\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"order_status\",\n",
        "      \"patterns\": [\n",
        "        \"Where is my order?\",\n",
        "        \"Status of my order\",\n",
        "        \"When will I get my order?\",\n",
        "        \"Tracking my package\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"I can check your order status. Please provide your order number.\",\n",
        "        \"For order status, please share your order ID.\",\n",
        "        \"I'll help you track your order. What's your order number?\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"refund\",\n",
        "      \"patterns\": [\n",
        "        \"I want a refund\",\n",
        "        \"How do I get my money back?\",\n",
        "        \"Return policy\",\n",
        "        \"Refund process\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"Our refund policy allows returns within 30 days. Please provide your order number.\",\n",
        "        \"I can help with your refund request. Could you share your order details?\",\n",
        "        \"For refunds, we need your order information to process the request.\"\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"payment_issue\",\n",
        "      \"patterns\": [\n",
        "        \"Payment failed\",\n",
        "        \"My payment didn't go through\",\n",
        "        \"Charge declined\",\n",
        "        \"Payment problem\"\n",
        "      ],\n",
        "      \"responses\": [\n",
        "        \"I'm sorry you're having payment issues. Let me check this for you.\",\n",
        "        \"Payment problems can occur due to various reasons. Let me assist you.\",\n",
        "        \"I'll help resolve your payment issue. Please provide more details.\"\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7AjbgoIZjXT",
        "outputId": "8b677853-8fab-4476-be8f-25af10dab89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting intents.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tokenizer.pickle and customer_care_gru_model.h5\n",
        "\n",
        "\n",
        "For a complete working example, you would need to train a model first. Here's a training script you can run first:"
      ],
      "metadata": {
        "id": "CoDrnuFcahlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_model.py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# Load intents\n",
        "with open('intents.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Prepare data\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "labels = []\n",
        "responses = []\n",
        "\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        training_sentences.append(pattern)\n",
        "        training_labels.append(intent['tag'])\n",
        "    responses.append(intent['responses'])\n",
        "\n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_sequences = pad_sequences(sequences, truncating='post', maxlen=20)\n",
        "\n",
        "# Encode labels\n",
        "lbl_encoder = LabelEncoder()\n",
        "lbl_encoder.fit(training_labels)\n",
        "training_labels = lbl_encoder.transform(training_labels)\n",
        "\n",
        "# Model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000, 64, input_length=20))\n",
        "model.add(GRU(64, return_sequences=True))\n",
        "model.add(GRU(32))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(labels), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(padded_sequences, np.array(training_labels), epochs=200)\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save('customer_care_gru_model.h5')\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(lbl_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"Model training complete. Files saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STJR6w80aR9e",
        "outputId": "fc5be65e-b6bd-4b8c-9dac-c2b7793c6ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step4: Run the chatbot**"
      ],
      "metadata": {
        "id": "9CaM5JOGbOil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2vR8Zvjzl8HdLPjZvriErsbtatq_2qKyvaKU9zE8XrZHXZnvT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBbka20IdPI7",
        "outputId": "c8efc85f-a963-45e9-c5f3-f043695538e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run customer_care_chatbot.py &\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsTSP5QlfNeg",
        "outputId": "61d50cee-6d42-4005-82aa-9107979cda1b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 293, in load\n",
            "    return loads(fp.read(),\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Stop previous tunnels (if any)\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit app in background\n",
        "os.system(\"streamlit run app.py &\")\n",
        "time.sleep(3)  # wait for server to start\n",
        "\n",
        "# Open ngrok tunnel to Streamlit on port 8501\n",
        "secure_url = ngrok.connect(8501, \"http\")\n",
        "print(f\"🔗 Your secure app is live at: {secure_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwF_0-03gHeq",
        "outputId": "0557751c-c498-4db7-90ac-521e4a84f3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Your secure app is live at: NgrokTunnel: \"https://61cf-34-106-153-171.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}